{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Measuring the relatedness of posts"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the machine learning point of view, raw text is useless. Only if we manage to transform it into meaningful numbers, can we feed it into our machine-learning algorithms ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "More rebust than edit distance is the so-called bag-of-word approach. For each word in the post, its occurrence is counted and noted in a vector. The vector is typically huge as it contains as many elements as the words that occur in the whole dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing - similarity measured as similar number of common words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting raw text into a bag-of-words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import scipy as sp\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(min_df=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CountVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "        dtype=<type 'numpy.int64'>, encoding=u'utf-8', input=u'content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "# the counting is done at word level\n",
    "print vectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[u'disk', u'distk', u'format', u'hard', u'how', u'my', u'problems', u'to']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "content = ['How to format my hard distk', 'Hard disk format problems']\n",
    "X = vectorizer.fit_transform(content)\n",
    "vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 1]\n",
      " [1 0]\n",
      " [1 1]\n",
      " [1 1]\n",
      " [1 0]\n",
      " [1 0]\n",
      " [0 1]\n",
      " [1 0]]\n"
     ]
    }
   ],
   "source": [
    "print X.toarray().transpose()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Counting words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this post dataset, we want to find the most similar post forthe short post \"imaging databases\". "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 25\n",
      "[[1 1 0 1 0 0 0 0 1 1 1 1 1 0 1 1 0 1 0 0 0 0 1 1 1]\n",
      " [0 0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0]\n",
      " [0 0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 0 0 0 0 0 3 0 0 0]\n",
      " [0 0 1 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 1 0 1 0 0 0 0]\n",
      " [0 0 0 0 0 1 1 1 0 0 0 0 0 1 0 0 1 0 0 1 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# feed CountVectorizer with the posts \n",
    "\n",
    "DIR = r\"../Building_ML_System/1400OS_03_Codes/data/toy\"\n",
    "# print os.listdir(DIR)\n",
    "posts = [open(os.path.join(DIR,f)).read() for f in os.listdir(DIR)]\n",
    "# print posts\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "# print X_train\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "print X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[u'about', u'actually', u'capabilities', u'contains', u'data', u'databases', u'images', u'imaging', u'interesting', u'is', u'it', u'learning', u'machine', u'most', u'much', u'not', u'permanently', u'post', u'provide', u'safe', u'storage', u'store', u'stuff', u'this', u'toy']\n"
     ]
    }
   ],
   "source": [
    "# Now we have five posts with a total of 25 different words\n",
    "print vectorizer.get_feature_names()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 5)\t1\n",
      "  (0, 7)\t1\n"
     ]
    }
   ],
   "source": [
    "# Now we can vectorize our new post as follows\n",
    "new_post = 'imaging databases'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print new_post_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0 0 0 0 0 1 0 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# we can access full ndarray as follows :\n",
    "print new_post_vec.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# For the similarity measurement(the naive one) we calculate the Euclidean \n",
    "# distance between the count vectors of the new post and all the old posts\n",
    "\n",
    "def dist_raw(v1, v2) :\n",
    "    delta = v1 - v2\n",
    "    return sp.linalg.norm(delta.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=4.00: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=1.41: Imaging databases store data.\n",
      "=== Post 2 with dist=5.10: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "=== Post 3 with dist=1.73: Imaging databases provide storage capabilities.\n",
      "=== Post 4 with dist=2.00: Most imaging databases safe images permanently.\n",
      "Best post is 1 with dist = 1.41\n"
     ]
    }
   ],
   "source": [
    "dist = dist_raw\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    \n",
    "    if post == new_post:\n",
    "        continue\n",
    "    \n",
    "    post_vec = X_train.getrow(i)    \n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    \n",
    "    print \"=== Post %i with dist=%.2f: %s\" %(i, d, post)\n",
    "    if d < best_dist:\n",
    "        best_dist = d \n",
    "        best_i = i\n",
    "print \"Best post is %i with dist = %.2f\" %(best_i, best_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normalizing the word count vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.77: Imaging databases store data.\n",
      "=== Post 2 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "=== Post 3 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 4 with dist=0.92: Most imaging databases safe images permanently.\n",
      "Best post is 1 with dist = 0.77\n"
     ]
    }
   ],
   "source": [
    "# extend dist_raw to calculate the vector distance on normalized ones\n",
    "def dist_norm(v1, v2):\n",
    "    v1_normalized = v1 / sp.linalg.norm(v1.toarray())\n",
    "    v2_normalized = v2 / sp.linalg.norm(v2.toarray())\n",
    "\n",
    "    delta = v1_normalized - v2_normalized\n",
    "    \n",
    "    return sp.linalg.norm(delta.toarray())\n",
    "\n",
    "dist = dist_norm\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    \n",
    "    if post == new_post:\n",
    "        continue\n",
    "    \n",
    "    post_vec = X_train.getrow(i)\n",
    "    #print post_vec\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    \n",
    "    print \"=== Post %i with dist=%.2f: %s\" %(i, d, post)\n",
    "    if d < best_dist:\n",
    "        best_dist = d \n",
    "        best_i = i\n",
    "print \"Best post is %i with dist = %.2f\" %(best_i, best_dist)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Removing less important words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Words such as \"the\" appear very often in all sorts of different contexts, and these words are called stop words. They do not carry as much information and thus should NOT be weighted as much as words such as \"images\"(that dont occur often in different contexts). The best option would be to remove all words that are so frequent that thet do not help to distingush between different texts. T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# this is a common step in text processing, there is a simple parameter in \n",
    "# CountVectorizer to achieve this :\n",
    "\n",
    "vectorizer = CountVectorizer(min_df=1, stop_words='english')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you have a clear picture of what kind of stop words you would want to remove, you can also pass a list of them. Setting stop_words = 'english' will use a set of 318 English stop words. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['a', 'about', 'above', 'across', 'after', 'afterwards', 'again', 'against', 'all', 'almost', 'alone', 'along', 'already', 'also', 'although', 'always', 'am', 'among', 'amongst', 'amoungst']\n"
     ]
    }
   ],
   "source": [
    "print sorted(vectorizer.get_stop_words())[0:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 18\n",
      "[[1 0 1 0 0 0 0 1 1 1 0 1 0 0 0 0 1 1]\n",
      " [0 0 0 1 1 0 1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 3 3 0 3 0 0 0 0 0 0 0 0 3 0 0]\n",
      " [0 1 0 0 1 0 1 0 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 1 1 1 0 0 0 1 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "# access the new word list with the new vectroizer\n",
    "X_train = vectorizer.fit_transform(posts)\n",
    "# print X_train\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "print X_train.toarray()\n",
    "#print X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.77: Imaging databases store data.\n",
      "=== Post 2 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "=== Post 3 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 4 with dist=0.86: Most imaging databases safe images permanently.\n",
      "Best post is 1 with dist = 0.77\n"
     ]
    }
   ],
   "source": [
    "# Now we can vectorize our new post as follows\n",
    "new_post = 'imaging databases'\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "\n",
    "dist = dist_norm\n",
    "best_doc = None\n",
    "best_dist = sys.maxint\n",
    "best_i = None\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    \n",
    "    if post == new_post:\n",
    "        continue\n",
    "    \n",
    "    post_vec = X_train.getrow(i)    \n",
    "    d = dist(post_vec, new_post_vec)\n",
    "    \n",
    "    print \"=== Post %i with dist=%.2f: %s\" %(i, d, post)\n",
    "    if d < best_dist:\n",
    "        best_dist = d \n",
    "        best_i = i\n",
    "print \"Best post is %i with dist = %.2f\" %(best_i, best_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "one thing is still missing. We count similiar words in different variants as different words. for instance, constrains \"imaging\" and \"images\". It would make sence to count them together. After all, it is the same concept \n",
    "they are referring to "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We need a function that reduces words to their specific word term. Scikit does not contain a stemmer by default. NLTK(Natural Language Toolkit) provides a stemmer that we can easily plug into CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import nltk.stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "s = nltk.stem.SnowballStemmer('english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'databas'"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"database\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'databas'"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem('databases')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'buy'"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buys\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "u'buy'"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s.stem(\"buying\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extending the vectorizer with NLTK's stemmer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to stem the posts before we feed them into CountVectorizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "english_stemmer = nltk.stem.SnowballStemmer('english')\n",
    "\n",
    "class StemmedCountVectorizer(CountVectorizer) :\n",
    "    def build_analyzer(self) :\n",
    "        analyzer = super(StemmedCountVectorizer, self).build_analyzer()\n",
    "        return lambda doc : (english_stemmer.stem(w) for w in analyzer(doc))\n",
    "\n",
    "vectorizer = StemmedCountVectorizer(min_df=1, stop_words = 'english')    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n",
      "[[0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0]]\n",
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'safe', u'storag', u'store', u'stuff', u'toy']\n",
      "[[1 0 1 0 0 0 1 1 1 0 1 0 0 0 0 1 1]\n",
      " [0 0 0 1 1 1 0 0 0 0 0 0 0 0 1 0 0]\n",
      " [0 0 0 3 3 3 0 0 0 0 0 0 0 0 3 0 0]\n",
      " [0 1 0 0 1 1 0 0 0 0 0 1 0 1 0 0 0]\n",
      " [0 0 0 0 1 2 0 0 0 1 0 0 1 0 0 0 0]]\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec.toarray())\n",
    "print(vectorizer.get_feature_names())\n",
    "print X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.77: Imaging databases store data.\n",
      "=== Post 2 with dist=0.77: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "=== Post 3 with dist=0.86: Imaging databases provide storage capabilities.\n",
      "=== Post 4 with dist=0.63: Most imaging databases safe images permanently.\n",
      "Best post is 4 with dist=0.63\n"
     ]
    }
   ],
   "source": [
    "dist = dist_norm\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "\n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stop words on steroids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a reasonalbe way to extract a compact vector from a noisy \n",
    "textaual post, let us set step back for a while to think about what the feature values actually mean. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The feature values simply count occurrences of terms in a post.We silently assumed that higher values for a term also mean that the term is of greater importance to the given post. But what about 'subject'(not the stop word) natually occurs in each and every single post ? We can solve this problem by setting the max_df = 0.9, this will remove all words that occur ini more than 90% of all posts.But there is a problem, hwoever we set it, there will always be the problem that some terms are just more discriminative than others. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can only be solved by counting term frequencies for every post and discounting those that appear in many posts. In other words, we want a high value for a given term in a given value if that term occurs often in that particular post and very realy anywhere else."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is TF-IDF(term frequency-inverse document frequency) dose "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tfidf(t, d, D):\n",
    "    tf = float(d.count(t)) / sum(d.count(w) for w in set(d))\n",
    "    idf = sp.log(float(len(D)) / (len([doc for doc in D if t in doc])))\n",
    "    return tf * idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a, abb, abc = [\"a\"],[\"a\",\"b\",\"b\"],[\"a\",\"b\",\"c\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "D = [a, abb, abc]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.0\n",
      "0.0\n",
      "0.135155036036\n",
      "0.366204096223\n"
     ]
    }
   ],
   "source": [
    "print tfidf(\"a\", a, D)\n",
    "print tfidf(\"a\", abc, D)\n",
    "print tfidf(\"b\", abc, D)\n",
    "print tfidf(\"c\", abc, D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(StemmedTfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: (english_stemmer.stem(w) for w in analyzer(doc))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Our achievements and goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our current text preprocessing phase includes the following steps :\n",
    "    1. Tokenizing the text.\n",
    "    2. Throwing away words that occur too often \n",
    "    3. Throwing away words that occur so seldom \n",
    "    4. Counting the remaining words\n",
    "    5. Calculating TF-IDF values from the counts, consider the whole text corpus\n",
    "With this process, we are able to convert a bunch of noisy text into a concise representation of feature values\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StemmedTfidfVectorizer(analyzer=u'word', binary=False, decode_error=u'strict',\n",
      "            dtype=<type 'numpy.int64'>, encoding=u'utf-8',\n",
      "            input=u'content', lowercase=True, max_df=1.0,\n",
      "            max_features=None, min_df=1, ngram_range=(1, 1), norm=u'l2',\n",
      "            preprocessor=None, smooth_idf=True, stop_words='english',\n",
      "            strip_accents=None, sublinear_tf=False,\n",
      "            token_pattern=u'(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None,\n",
      "            use_idf=True, vocabulary=None)\n"
     ]
    }
   ],
   "source": [
    "vectorizer = StemmedTfidfVectorizer(min_df=1, stop_words='english')\n",
    "print(vectorizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#samples: 5, #features: 17\n",
      "[[ 0.          0.          0.          0.          0.70710678  0.70710678\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]]\n",
      "[u'actual', u'capabl', u'contain', u'data', u'databas', u'imag', u'interest', u'learn', u'machin', u'perman', u'post', u'provid', u'safe', u'storag', u'store', u'stuff', u'toy']\n",
      "[[ 0.35355339  0.          0.35355339  0.          0.          0.\n",
      "   0.35355339  0.35355339  0.35355339  0.          0.35355339  0.          0.\n",
      "   0.          0.          0.35355339  0.35355339]\n",
      " [ 0.          0.          0.          0.57974759  0.40483667  0.40483667\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.57974759  0.          0.        ]\n",
      " [ 0.          0.          0.          0.57974759  0.40483667  0.40483667\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.57974759  0.          0.        ]\n",
      " [ 0.          0.52451722  0.          0.          0.29550385  0.29550385\n",
      "   0.          0.          0.          0.          0.          0.52451722\n",
      "   0.          0.52451722  0.          0.          0.        ]\n",
      " [ 0.          0.          0.          0.          0.29746628  0.59493255\n",
      "   0.          0.          0.          0.52800051  0.          0.\n",
      "   0.52800051  0.          0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "X_train = vectorizer.fit_transform(posts)\n",
    "\n",
    "num_samples, num_features = X_train.shape\n",
    "print(\"#samples: %d, #features: %d\" % (num_samples, num_features))\n",
    "\n",
    "new_post_vec = vectorizer.transform([new_post])\n",
    "print(new_post_vec.toarray())\n",
    "print(vectorizer.get_feature_names())\n",
    "print X_train.toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Post 0 with dist=1.41: This is a toy post about machine learning. Actually, it contains not much interesting stuff.\n",
      "=== Post 1 with dist=0.92: Imaging databases store data.\n",
      "=== Post 2 with dist=0.92: Imaging databases store data. Imaging databases store data. Imaging databases store data.\n",
      "=== Post 3 with dist=1.08: Imaging databases provide storage capabilities.\n",
      "=== Post 4 with dist=0.86: Most imaging databases safe images permanently.\n",
      "Best post is 4 with dist=0.86\n"
     ]
    }
   ],
   "source": [
    "dist = dist_norm\n",
    "\n",
    "best_dist = sys.maxsize\n",
    "best_i = None\n",
    "\n",
    "for i in range(0, num_samples):\n",
    "    post = posts[i]\n",
    "    if post == new_post:\n",
    "        continue\n",
    "    post_vec = X_train.getrow(i)\n",
    "    d = dist(post_vec, new_post_vec)\n",
    "\n",
    "    print(\"=== Post %i with dist=%.2f: %s\" % (i, d, post))\n",
    "\n",
    "    if d < best_dist:\n",
    "        best_dist = d\n",
    "        best_i = i\n",
    "        \n",
    "print(\"Best post is %i with dist=%.2f\" % (best_i, best_dist))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## summarise of bag-of-word :"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as simple and as powerful as the bag-of-words approach with its extensions is, it has some drawbacks that we should be aware of. \n",
    "- It does not cover word relations. With the previous vectorization approach, the text \"Car his wall\" and \"Wall hits car\" will both have the same feature vector.\n",
    "- It does not capture negations correctly. For instance, the text \"I will eat ice cream\" and \"I will not eat ice cream\" will look very similar by means of their feature vectors, although they contain quite the opposite meaning. This problem can be easily changed by not only counting individual words(unigrams) but also considering bigrams / trigrarms.\n",
    "- It totally fails with misspelled words. Although the \"database\" and \"databas\" convey the same meaning, our approach will treat them as totally different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
